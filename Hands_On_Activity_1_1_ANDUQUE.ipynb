{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KurtGabrielAnduque/DSA-CPE010CPE21S1/blob/main/Hands_On_Activity_1_1_ANDUQUE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On-Activity-1.1"
      ],
      "metadata": {
        "id": "I6Q6SR6o27Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective(s)\n",
        "The objective of this activity is to familiarize students with PySpark and Google Colab by setting up a foundational big data environment suitable for distributed data processing."
      ],
      "metadata": {
        "id": "G5VxqWsX5EK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intended Learning Outcomes\n",
        "\n",
        "At the end of this activity, the students should be able to:\n",
        "\n",
        "\n",
        "*   Analyze how PySpark handles data processing compared to traditional tools.\n",
        "*   Evaluate the benefits of distributed computing using simulated large-scale operations.\n",
        "*   Create a basic data processing workflow using PySpark DataFrames on Google Colab\n",
        "\n"
      ],
      "metadata": {
        "id": "13hXRFk75CCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "\n",
        "**What is PySpark?**\n",
        "\n",
        "Apache Spark is an open-source, distributed computing system designed for fast processing of large-scale data. PySpark is the Python interface for Apache Spark. It handles large datasets efficiently with parallel computation in Python workflows, ideal for batch processing, real-time streaming, machine learning, data analytics, and SQL querying. PySpark supports industries like finance, healthcare, and e-commerce with speed and scalability.\n",
        "\n",
        "\n",
        "**Spark Cluster**\n",
        "\n",
        "\n",
        "A key component of working with PySpark is clusters. A Spark cluster is a group of computers (nodes) that collaboratively process large datasets using Apache Spark, with a master node coordinating multiple worker nodes. This architecture enables distributed processing. The master node manages resources and tasks, while worker nodes execute assigned compute tasks.\n",
        "\n",
        "\n",
        "**Spark Session**\n",
        "\n",
        "\n",
        "A SparkSession is the entry point into PySpark, enabling interaction with Apache Spark's core capabilities. It allows us to execute queries, process data, and manage resources in the Spark cluster."
      ],
      "metadata": {
        "id": "m9qJStw-5CcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Materials and Equipment\n",
        "\n",
        "\n",
        "*   Internet connection\n",
        "*   Google Colab\n",
        "*   Personal Computer\n",
        "\n"
      ],
      "metadata": {
        "id": "2F2HiO2G5Ce-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procedure"
      ],
      "metadata": {
        "id": "cIiQRyJi53bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Google Colab and Initializationof SparkSession"
      ],
      "metadata": {
        "id": "jL_NQHqo56Mh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qWF0Whm2rji",
        "outputId": "fe0d111f-8213-40f9-99d7-552c62ef6125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7bac61f947d0>\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "my_spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
        "print(my_spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "Bn9SmAXD6B-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DhZ58BYM77m7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443c5515-19b6-450f-d929-2ce4e81bf9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "username_df = my_spark.read.csv(\"/content/drive/MyDrive/(1st Semester AY 2025 to 2026) CPE 032/CPE31S4/username.csv\", header=True, inferSchema=True) ## TODO: Change path according to your folder setup\n",
        "username_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "_O0jiFy526yS",
        "outputId": "b26980b3-ee20-4866-e129-2a22ed2908e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/drive/MyDrive/(1st Semester AY 2025 to 2026) CPE 032/CPE31S4/username.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3096051387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0musername_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/(1st Semester AY 2025 to 2026) CPE 032/CPE31S4/username.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0musername_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/drive/MyDrive/(1st Semester AY 2025 to 2026) CPE 032/CPE31S4/username.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow up question, what happens when you run this code?"
      ],
      "metadata": {
        "id": "WqH3y_us3WRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "P0KwJov629C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "count the number of rows"
      ],
      "metadata": {
        "id": "VPGntgn_6HzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username_df.count()"
      ],
      "metadata": {
        "id": "Qmk1blM94dc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Filtering of dataframe"
      ],
      "metadata": {
        "id": "tkX-1uo96LGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = username_df.filter(username_df[\"Identifier\"] > 1002)"
      ],
      "metadata": {
        "id": "eG7GTUZt4iXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.show()"
      ],
      "metadata": {
        "id": "Dd3ehuD14pXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whatâ€™s the output?"
      ],
      "metadata": {
        "id": "cHU-3NFq6PTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "Ll1KLA814ocv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregating and Grouping"
      ],
      "metadata": {
        "id": "aADowrJf6Ruf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "username_df.groupBy(\"Username\").agg(avg(\"Identifier\")).show()"
      ],
      "metadata": {
        "id": "w2oY3d2m4yIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarize the procedure conducted"
      ],
      "metadata": {
        "id": "YJU4Pnsw6ZlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "-WNRDkgC6fX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supplementary Activity"
      ],
      "metadata": {
        "id": "1ugAzcrO6gDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create a dataframe\n",
        "2. Use the old SparkSession instead of creating a new one\n",
        "3. Load the Employee_salaries.csv\n",
        "4. Filter the Employees based on their gender\n",
        "5. Group the Employees based on their Gender and Average their Salaries\n",
        "6. Compute annual salary for each employee.\n",
        "7. Sort the result and display the highest average."
      ],
      "metadata": {
        "id": "gn4pQKOg6p3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note that for each number in the supplementary activity (1-7), provide and explanation of the procedure, output, and/or analysis*"
      ],
      "metadata": {
        "id": "OGSE-Xi77CTA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGplkhJo6zEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOA Conclusion"
      ],
      "metadata": {
        "id": "swyC_Zk96rvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your answer here**"
      ],
      "metadata": {
        "id": "QsZDae4-6wRH"
      }
    }
  ]
}